## Brief Summary
## é€‚åˆRLçš„é—®é¢˜
1.sequential decision makingé—®é¢˜ï¼Œè¿›è¡Œåºåˆ—å†³ç­–ï¼ˆactionï¼‰ï¼Œå¹¶æœ‰rewardå¯ä»¥è¡¡é‡actionçš„å¥½åç¨‹åº¦ã€‚ æ–‡æœ¬ç”Ÿæˆé¢†åŸŸæ²¡æœ‰åˆé€‚çš„æŒ‡æ ‡ï¼Œå› æ­¤rewardå¾ˆéš¾è®¾è®¡ã€‚
2.æ— label,æ¯ä¸ªactionç”±è¯•é”™åé€‰æ‹©
### Seq2seq
Sequence Generative Adversarial Nets with Policy Gradient
æ¨æ•²ç½‘ç»œ
### RLè¾…åŠ©å­¦ä¹ è¯­ä¹‰å‘é‡
motivationæ˜¯å¿«é€Ÿé˜…è¯»
[1711.02085] Neural Speed Reading via Skim-RNN

å­¦ä¹ æ–‡ç« ç»“æ„ï¼Œword-levelçš„æ¯ä¸€ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œåˆ¤æ–­è¯ç»„èµ·å§‹ä½ç½®ï¼Œä»¥è¯ç»„å‘é‡ä¸ºå•ä½è¾“å…¥æ®µè½çº§çš„LSTMä¸­ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ–‡ç« å‘é‡
æ›´åŠ å‡†ç¡®æ•æ‰åˆ°è¯/å¥çš„æ„æ€
Deep reinforcement learning for dialogue generation
Learning to compose words into sentences with reinforcement learning
### ä½¿ç”¨RLå¯¹æ¨¡å‹è¿›è¡Œè°ƒå‚
è°ƒæ•´RNN/CNNçš„å±‚æ•°æˆ–ç»´åº¦ï¼ˆè°·æ­ŒNMTç”¨äº†å¼ºåŒ–å­¦ä¹ è°ƒå‚æ•°ï¼‰
### Text Gameç±»åˆ«
Language Understanding for Text-based Games using Deep Reinforcement Learning

## RLä¸NLP
ä¼˜åŠ¿
1.åœ¨ç¦»æ•£ç©ºé—´çš„æ–‡æœ¬ç”Ÿæˆå’Œåºåˆ—å†³ç­–
2.goal orientedçš„å¯¹è¯ç³»ç»Ÿ
3.RLçš„ä¼˜åŠ¿ï¼Œå¦‚å¯ä»¥å…‹æœå…¶ä»–ç›®æ ‡å‡½æ•°å¦‚MLEçš„ç¼ºé™·ï¼Œå¯ä»¥æ¨¡æ‹Ÿå¤§é‡æ ·æœ¬ï¼Œæˆ–è€…å€ŸåŠ©å…ˆå‰ç»éªŒè¿›è¡Œå­¦ä¹ ï¼ˆå¦‚DQNï¼‰ç­‰
åŠ£åŠ¿
å¼ºåŒ–å­¦ä¹ å¾ˆå¤šæ—¶å€™è®­ç»ƒæ˜¯å¾ˆä¸ç¨³å®šçš„ï¼Œæœ¬èº«ç®—æ³•åƒé«˜æ–¹å·®ï¼Œéš¾ä¼˜åŒ–

# ICML 17 RL IN NLP tutorial ç¬”è®°
## Challenges
- Sparse reward (few feedback when making decisions)
- Difficulty in reward function design
- High-dimensional action space
- High variance in training RL algorithms

## Strengthens
- Weak supervision without explicit annotations
- Trial-and-error: probabilistic exploring
- Accumulative rewards: encoding expert/prior knowledge in reward design

## Forms
- Immediate rewards
ç±»ä¼¼ teacher forcing 
- Delayed rewards
 Comparing with gold standard: BLEU\ACC\F1 
 classifier: likelihood 
 Prior/domain expertise:sparsity or continuity 
 
# Reinforcement Learning for NLP ç¬”è®°
## Approaches to RL
### Value-based RL
- Estimate the optimal value function Q(s ,a)
- This is the maximum value achievable under any policy
Value-based RL learns near deterministic policy

### Policy-based RL
- Search directly for the optimal policy PI
- This is the policy achieving maximum future reward
#### Advantages:
Better convergence properties
Effective in high-dimensional or continuous action spacesCan learn stochastic policies
#### Disdvantages:
Typically converge to a local rather than global optimum
Evaluating a policy is typically inefficient and high variance
#### Extension:
Combine with value estimation
Critic: Updates action-value function parameters
Actor: Updates policy parameters in direction suggested by critic

### Model-based RL
- Build a model of the environment
- Plan (e.g. by lookahead) using model
# CS224N ç¬”è®°
- Experience is a sequence of observations, actions, rewards
- State is a summary of experience
- RL Agent
Policy: agentâ€™s behavior function
Value function: how good would be each state and/or action
Model: agentâ€™s prediction/representation of the environment
## What is Deep RL?

â— Use deep neural network to approximate
â—‹ Policy
â—‹ Value function
â—‹ Model

â— Optimized by SGD
## RL in NLP

â— Article summarization
** A Deep Reinforced Model for Abstractive Summarization**
Reward: ROUGH score

â— Question answering
**DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR
QUESTION ANSWERING**
Introduce F1 score as extra objective combining with
traditional cross entropy loss

â— Dialogue generation
** Deep Reinforcement Learning for Dialogue Generation**
Action: infinite since arbitrary-length sequences can be generated.
State: A state is denoted by the previous two dialogue turns [ğ‘i, ğ‘i].
Reward: Ease of answering, Information Flow and Semantic Coherence

â— Dialogue System
â— Knowledge-based QA
â— Machine Translation
â— Text generation

## Resources

RL for NLP(papers)   https://github.com/sunxs1101/RL-Dialog/blob/master/note.md

RL for NL(A review)  http://coai.cs.tsinghua.edu.cn/hml/media/files/reinforcement_learning-in-NLP-Huangminlie.pdf

RL for NL(A review) http://users.umiacs.umd.edu/~jbg/teaching/CSCI_7000/11a.pdf

RL for NL(A review) https://www.youtube.com/watch?v=F1hZfoh-wX4

RL for NL(A review) https://web.stanford.edu/class/cs224n/lectures/lecture16-guest.pdf

RL for NLG(Blog)    https://zhuanlan.zhihu.com/p/22385421
